{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "from collections import Counter\n",
    "import mysql.connector\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPTを使うのでAPIキーを設定\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "gpt_client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB接続処理\n",
    "# Neo4jに接続\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"abcd7890\"\n",
    "\n",
    "# ドライバを作成\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPTを使用して意味のある文章を生成\n",
    "def get_gpt_result(prompt):\n",
    "    # Replace `gpt_client` with proper OpenAI API calls\n",
    "    # Adjust this based on how your API client is initialized\n",
    "    llm = ChatOpenAI(model=\"gpt-4\")  # Or \"gpt-3.5-turbo\"\n",
    "    return llm.predict(prompt).strip()\n",
    "\n",
    "# 作業時間を分（min）に揃える\n",
    "def extract_minutes(text):\n",
    "    prompt = f\"\"\"\n",
    "Convert a given total_time string (e.g., \"1 hr 45 min\") into the total time in minutes. Ensure to handle cases with both \"hr\" and \"min\" or only \"hr\" or \"min\". Return the result as an integer representing the total minutes.\n",
    "\n",
    "Input:\n",
    "{text}\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "    return int(get_gpt_result(prompt))\n",
    "\n",
    "\n",
    "# 作業時間を材料から推測する\n",
    "def estimate_time_from_title_and_ingredients(title, ingredients):\n",
    "    \"\"\"\n",
    "    Estimate total time required based on title and ingredients.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Estimate the total time required to complete a task based on its title and detailed instructions. \n",
    "Consider the complexity, number of steps, and common preparation time required.\n",
    "Return the total time as an integer value representing the number of minutes.\n",
    "\n",
    "Input:\n",
    "title: {title}\n",
    "ingredients: {', '.join(ingredients)}\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "    return int(get_gpt_result(prompt))\n",
    "\n",
    "# 作業時間を作業手順から推測する\n",
    "def estimate_time_from_title_and_instructions(title, instructions):\n",
    "    \"\"\"\n",
    "    Estimate total time required based on title and instructions.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Estimate the total time required to complete a task based on its title and detailed instructions. \n",
    "Consider the complexity, number of steps, and common preparation time required.\n",
    "Return the total time as an integer value representing the number of minutes.\n",
    "\n",
    "Input:\n",
    "title: {title}\n",
    "instructions: {instructions}\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "    return int(get_gpt_result(prompt))\n",
    "\n",
    "def fetch_webpage_content(url):\n",
    "    \"\"\"\n",
    "    Fetch HTML content from the URL and extract visible text.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    return text\n",
    "\n",
    "# webページから情報を抽出する\n",
    "\n",
    "# 食材名をURLから抽出する\n",
    "def extract_ingredient_names_from_list(ingredients):\n",
    "    # リストをカンマ区切りの文字列にフォーマット\n",
    "    formatted_string = \", \".join(ingredients)\n",
    "    # フォーマットされた文字列を別の関数に渡す\n",
    "    return extract_ingredient_names_from_text(formatted_string)\n",
    "\n",
    "# 食材名をURLから抽出する\n",
    "def extract_ingredient_names_from_text(content):\n",
    "    \"\"\"\n",
    "    Use LangChain to extract ingredient names from the given URL's content.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 2: Define the prompt template\n",
    "    prompt_template = \"\"\"\n",
    "    I want you to act as an ingredient extractor. Given the text content of a webpage, \n",
    "    extract and return a clean, deduplicated array of ingredient names mentioned in the content. \n",
    "    Do not include unrelated text or additional information. \n",
    "    Extract the ingredient names from the text below. Follow these rules:\n",
    "    1. Remove any quantities, units, or modifiers (e.g., \"2 cups\", \"1/2 tsp\").\n",
    "    2. Simplify descriptive phrases by removing extra adjectives or specific modifiers. For example:\n",
    "      - \"Extra-virgin olive oil\" → \"olive oil\"\n",
    "      - \"Freshly ground black pepper\" → \"pepper\"\n",
    "      - \"Dark brown sugar\" → \"sugar\"\n",
    "    3. Convert multiple forms of the same ingredient into its singular, generalized form (e.g., \"apples\" → \"apple\").\n",
    "    4. Replace specific brand names, product names, or unique ingredient names with simple, general terms (e.g., \"Epsom salt\" → \"salt\").\n",
    "    5. Ensure that the extracted ingredients are clean, deduplicated, and in singular form.\n",
    "    6. Standardize formatting for compound ingredients by adding proper spacing after commas. For example:\n",
    "      - \"Salt, Pepper\" → \"salt\", \"pepper\"\n",
    "    7. Convert subtypes of ingredients into their base types. For example:\n",
    "      - \"lemon juice\" → \"lemon\"\n",
    "      - \"chicken broth\" → \"chicken\"\n",
    "      - \"almond milk\" → \"almond\"\n",
    "    If no ingredients are found, return an empty array.\n",
    "\n",
    "    Here is the text content: \n",
    "    {content}\n",
    "    \n",
    "    Output:\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 3: Use ChatGPT to extract ingredients\n",
    "    prompt = prompt_template.format(content=content)\n",
    "    result = get_gpt_result(prompt)\n",
    "    ingredient = json.loads(result) or []\n",
    "    return ingredient\n",
    "\n",
    "\n",
    "# インストラクションを抽出する\n",
    "def extract_instructions_from_webpage(content):\n",
    "    \"\"\"\n",
    "    Extract cooking instructions from a webpage's content and return them as a JSON array \n",
    "    in the format [\"instruction\", \"instruction\", ...].\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "I want you to act as a cooking instruction extractor. Your task is to extract the detailed steps or instructions for a recipe from the provided text content of a webpage. The extracted instructions should be:\n",
    "\n",
    "1. Returned as a JSON array, where each element is a string (e.g., \"Preheat the oven to 350°F\").\n",
    "2. Exclude any ingredients, introductions, or unrelated information (e.g., advertisements or nutritional facts).\n",
    "3. Focus only on clear, actionable cooking steps (e.g., \"Preheat the oven to 350°F\" or \"Chop the onions and sauté them until golden brown\").\n",
    "4. Remove \"Step 1:\" or similar prefixes from the instructions.\n",
    "5. If no cooking instructions are found, return an empty JSON array ([]).\n",
    "\n",
    "Here is the content of the webpage:\n",
    "{content}\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "    result = get_gpt_result(prompt)\n",
    "    return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストフォーマットの指定\n",
    "def format_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = text.replace('(', '_')\n",
    "    text = text.replace(')', '_')\n",
    "    text = text.replace(\"/\", '_')\n",
    "    text = text.replace(\";\", '_')\n",
    "    text = text.replace(\":\", '_')\n",
    "    text = text.replace(\"&\", '_')\n",
    "    text = text.replace(\"[\", '')\n",
    "    text = text.replace(\"]\", '')    \n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace('<', '')\n",
    "    text = text.replace('>', '')\n",
    "    text = text.replace(', ', ',')\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace(',', '_')\n",
    "    text = text.replace('-', ' ')\n",
    "    text = text.replace(' ', '_')\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.replace('%', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"/t\", '')\n",
    "    text = text.replace(\"\\\\\", '')\n",
    "    text = text.replace(\"é\", '')\n",
    "    text = text.replace(\"ç\", '')\n",
    "    text = text.replace(\"+\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"ã\", '')\n",
    "    text = text.replace(\"â\", '')\n",
    "    text = text.replace(\"ƒ\", '')\n",
    "    text = text.replace(\"€\", '')\n",
    "    text = text.replace('”', '')\n",
    "    text = text.replace('`', '')\n",
    "    text = text.replace('!', '')\n",
    "    text = text.replace('|', '')\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "    text = re.sub(r'\\s+', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def escape_sql_string(text):\n",
    "    text = text.rstrip(\"\\\\\")\n",
    "    return text.replace(\"'\", \"''\")\n",
    "\n",
    "def clean_title(text):\n",
    "    text = text.replace(\"ã\", '')\n",
    "    text = text.replace(\"â\", '')\n",
    "    text = text.replace(\"ƒ\", '')\n",
    "    text = text.replace(\"€\", '')\n",
    "    text = text.replace(\"é\", '')\n",
    "    text = text.replace(\"ç\", '')\n",
    "    # UTF-8にエンコードし、デコード時にエラーを無視して文字化けを削除\n",
    "    text = text.encode('utf-8', 'replace').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークン化して保存する\n",
    "def tokenize_save(key_name, sentences):\n",
    "    # NumPyの配列に変換\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        if sentence is None:\n",
    "            continue\n",
    "        \n",
    "        if isinstance(sentence, list):\n",
    "            sentence = \" \".join(sentence)\n",
    "            \n",
    "        # 小文字化し、特殊文字や句読点を削除してトークン化\n",
    "        words = re.findall(r'\\b[a-zA-Z0-9_]+\\b', sentence.lower())\n",
    "        tokens.append(words)\n",
    "        \n",
    "    np_sentences = np.array(tokens, dtype=object)\n",
    "\n",
    "    # NumPyの配列をdumpして保存\n",
    "    np.save(f\"../datas/word2_vec/{key_name}.npy\", np_sentences)\n",
    "    with open(f\"../datas/word2_vec/{key_name}.txt\", 'w') as f:\n",
    "        json.dump(tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_food(tx, search_term):\n",
    "    find_food_query = \"\"\"\n",
    "    CALL db.index.fulltext.queryNodes(\"food_sub_index_text_search\", $search_term)\n",
    "    YIELD node, score\n",
    "    ORDER BY score DESC, size(node.name)\n",
    "    LIMIT 1\n",
    "    RETURN node.id as node_id, node.name as node_name, node.flavor_vector as flavor_vector, node.word_vector as word_vector, score    \n",
    "    \"\"\"\n",
    "    return tx.run(find_food_query, search_term=search_term).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_OUTPUT_PATH = \"../data/formatted_json_recipe.json\"\n",
    "def get_json_files(file):\n",
    "    json_files = os.listdir(file)\n",
    "    return json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1388\n"
     ]
    }
   ],
   "source": [
    "unique_recipes = {}\n",
    "\n",
    "for file in get_json_files(\"../data/json_recipes/\"):\n",
    "    with open(f\"../data/json_recipes/{file}\") as f:\n",
    "        json_data = json.load(f)\n",
    "        if not 'recipes_results' in json_data or json_data['recipes_results'] is None:\n",
    "            continue\n",
    "        \n",
    "        for item in json_data[\"recipes_results\"]:\n",
    "            unique_recipes[item['title']] = item\n",
    "\n",
    "for file2 in get_json_files(\"../data/html_recipes/out_jsons/\"):\n",
    "    with open(f\"../data/html_recipes/out_jsons/{file2}\") as f:\n",
    "        json_data = json.load(f)\n",
    "        for item in json_data:\n",
    "            if not 'title' in item or item['ingredients'] == \"None\" or item['ingredients'] == []:\n",
    "                continue \n",
    "            json_input = {\n",
    "                \"title\": item[\"title\"],\n",
    "                \"ingredients\": item['ingredients'],\n",
    "                \"instructions\": item['instructions'],\n",
    "            }\n",
    "            unique_recipes[item['title']] = json_input\n",
    "\n",
    "print(len(unique_recipes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = {}\n",
    "OUTPUT_FILE_NAME = \"../data/formatted_json_recipe.json\"\n",
    "already_processed = []\n",
    "with open(OUTPUT_FILE_NAME, 'r') as f2:\n",
    "    recipes = json.load(f2)\n",
    "    already_processed =recipes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/fbyxhbrx6h3_4s4m15x3bly00000gn/T/ipykernel_14056/1199722153.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4\")  # Or \"gpt-3.5-turbo\"\n",
      "/var/folders/g4/fbyxhbrx6h3_4s4m15x3bly00000gn/T/ipykernel_14056/1199722153.py:6: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return llm.predict(prompt).strip()\n"
     ]
    }
   ],
   "source": [
    "for key, item in unique_recipes.items():\n",
    "    if not 'ingredients' in item or item['ingredients'] is None:\n",
    "        continue\n",
    "    \n",
    "    title = clean_title(item['title'])\n",
    "    if title in already_processed:\n",
    "        continue\n",
    "    \n",
    "    extracted_ingredient = []\n",
    "    extracted_instructions = []\n",
    "    if 'link' in item:\n",
    "        try:\n",
    "            content = fetch_webpage_content(item[\"link\"])\n",
    "            extracted_ingredient =  extract_ingredient_names_from_text(content)\n",
    "            extracted_instructions = extract_instructions_from_webpage(content)\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "    ingredient = extract_ingredient_names_from_list(item['ingredients'])\n",
    "    \n",
    "    sanitized_ingredients = list(set(extracted_ingredient + ingredient))\n",
    "    \n",
    "    instructions = []\n",
    "    if 'instructions' in item:\n",
    "        instructions = item['instructions']\n",
    "    else:\n",
    "        instructions = extracted_instructions\n",
    "    \n",
    "    total_time = \"\"\n",
    "    if \"total_time\" in item:\n",
    "        total_time = extract_minutes(item['total_time'])\n",
    "    elif 'instructions' in item:\n",
    "        total_time = estimate_time_from_title_and_instructions(item['title'], sanitized_ingredients)\n",
    "    else:\n",
    "        total_time = estimate_time_from_title_and_ingredients(item['title'], sanitized_ingredients)\n",
    "    \n",
    "    rating = \"\"\n",
    "    if \"rating\" in item:\n",
    "        rating = item['rating']\n",
    "    \n",
    "    reviews = \"\"\n",
    "    if \"reviews\" in item:\n",
    "        reviews = item['reviews']\n",
    "    else:\n",
    "        reviews = 0\n",
    "        \n",
    "    json_input = {\n",
    "        \"name\": title,\n",
    "        \"rating\": rating,\n",
    "        \"reviews\": reviews,\n",
    "        \"total_time\": total_time,\n",
    "        \"ingredients\": sanitized_ingredients,\n",
    "        \"instructions\": instructions\n",
    "    }\n",
    "    recipes[title] = json_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_FILE_NAME, 'w') as f2:\n",
    "    json.dump(recipes, f2, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipe-qa-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
